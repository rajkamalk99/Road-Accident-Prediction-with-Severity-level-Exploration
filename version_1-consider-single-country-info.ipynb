{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import numpy, pandas, matpltlib.pyplot, sklearn modules and seaborn\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom tqdm import tqdm\nimport gc\n\n%matplotlib inline\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\n# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn import preprocessing\nfrom catboost import CatBoostRegressor, Pool, cv, CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB \n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgbm\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def import_data():\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(\"/kaggle/input/us-accidents/US_Accidents_Dec19.csv\")\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the data\ndf = import_data()\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Start_Time and End_Time to datetypes\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n\n# Extract year, month, day, hour and weekday\ndf['Year']=df['Start_Time'].dt.year\ndf['Month']=df['Start_Time'].dt.strftime('%b')\ndf['Day']=df['Start_Time'].dt.day\ndf['Hour']=df['Start_Time'].dt.hour\ndf['Weekday']=df['Start_Time'].dt.strftime('%a')\n\n# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\ntd='Time_Duration(min)'\ndf[td]=round((df['End_Time']-df['Start_Time'])/np.timedelta64(1,'m'))\n# df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if there is any negative time_duration values\ndf[td][df[td]<=0]\n\n# Drop the rows with td<0\n\nneg_outliers =df[td]<=0\n\n# Set outliers to NAN\ndf[neg_outliers] = np.nan\n\n# Drop rows with negative td\ndf.dropna(subset=[td],axis=0,inplace=True)\n\n# Double check to make sure no more negative td\nprint(\"Double check to make sure no more negative td: \", df[td][df[td]<=0])\n\ndel neg_outliers\n\n# df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print time_duration information\nprint('Max time to clear an accident: {} minutes or {} hours or {} days; Min to clear an accident td: {} minutes.'.format(df[td].max(),round(df[td].max()/60), round(df[td].max()/60/24), df[td].min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the list of features to include in Machine Learning\nfeature_lst=['Source','TMC','Severity','Start_Lng','Start_Lat',\n             'Distance(mi)','Side','City','County','State','Timezone',\n             'Temperature(F)','Humidity(%)','Pressure(in)', 'Visibility(mi)', \n             'Wind_Direction','Weather_Condition','Amenity','Bump',\n             'Crossing','Give_Way','Junction','No_Exit','Railway',\n             'Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal',\n             'Turning_Loop','Sunrise_Sunset','Hour','Weekday', 'Time_Duration(min)'\n            ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the dataset to include only the selected features\ndf_sel =df[feature_lst].copy()\n# df_sel.info()\n\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data_stats(df):\n    zero_val = (df == 0.00).astype(int).sum(axis=0)\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n    mz_table = mz_table.rename(\n    columns = {0 : 'Zero Values', 1 : 'Missing(Null) Values', 2 : '% of Total Missing(Null) Values'})\n    mz_table['Total (Zero + Missing) Values'] = mz_table['Zero Values'] + mz_table['Missing(Null) Values']\n    mz_table['% Total (Zero + Missing) Values'] = 100 * mz_table['Total (Zero + Missing) Values'] / len(df)\n    mz_table['Data Type'] = df.dtypes\n    mz_table = mz_table[mz_table.iloc[:,1] != 0].sort_values('% of Total Missing(Null) Values', ascending=False).round(1)\n    print (\"There are \" + str(mz_table.shape[0]) + \" columns that have missing values out of \", len(df.columns), \" columns.\")\n    if int(mz_table.shape[0]) != 0:\n        return mz_table\n    else:\n        return None\n    \nfaulty_data = missing_data_stats(df_sel)\nfaulty_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_missing_values(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    f, ax = plt.subplots(figsize=(15, 6))\n    plt.xticks(rotation='90')\n    sns.barplot(x=missing_data.index, y=missing_data['Percent'])\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n\nplot_missing_values(df_sel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sel.dropna(subset=df_sel.columns[df_sel.isnull().mean()!=0], how='any', axis=0, inplace=True)\ndf_sel.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set county\ncounty='Montgomery'\n\n# Select the state of Pennsylvania\ndf_county=df_sel.loc[df_sel.County==county].copy()\ndf_county.drop('County',axis=1, inplace=True)\n# df_county.info()\n\ndel df_sel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_county.shape)\nprint(Counter(df_county.Severity))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate dummies for categorical data\n\ntarget = df_county[\"Severity\"]\n\ndf_county_ = df_county.drop([\"Severity\"], axis=1)\n\ndf_sel_dummies = pd.get_dummies(df_county_,drop_first=True)\n\nencoder = preprocessing.LabelEncoder()\nencoder.fit(target)\ntarget_ = encoder.transform(target)\n# convert integers to dummy variables (i.e. one hot encoded)\n# target_ = np_utils.to_categorical(target_)\n\n# df_sel_dummies.info()\n# target.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the target for the prediction\n# target='Severity'\n\n# Create arrays for the features and the response variable\n\n# set X and y\n# y = df_sel_dummies[target]\ny = target_\n# X = df_sel_dummies.drop(target, axis=1)\nX = df_sel_dummies\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of classification algorithms\nalgo_lst=['Logistic Regression',' K-Nearest Neighbors','Decision Trees','Random Forest']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_lst=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression\nlr = LogisticRegression(random_state=42)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest algorithm\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Randon forest algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf = svm.LinearSVC()\nsvm_clf.fit(X_train, y_train)\ny_pred = svm_clf.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[SVM algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_clf = GaussianNB()\nNB_clf.fit(X_train, y_train)\ny_pred=NB_clf.predict(np.array(X_test))\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Naive Bieas algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD_clf = SGDClassifier()\nSGD_clf.fit(X_train, y_train)\ny_pred= SGD_clf.predict(np.array(X_test))\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Shocastic Gradient Descent algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_train)\ny_pred = knn_clf.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[KNN algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_clf = XGBClassifier(\n#   learning_rate =0.1,\n#   n_estimators=1000,\n#   max_depth=5,\n#   min_child_weight=1,\n#   gamma=0,\n#   subsample=0.8,\n#   colsample_bytree=0.8,\n#   objective= 'multi:softmax',\n#   num_class =4,\n#   nthread=4,\n#   scale_pos_weight=1,\n#   seed=27)\n\n# xgb_clf.fit(X_train, y_train)\n# y_pred = xgb_clf.predict(X_test)\n\n# # Get the accuracy score\n# acc=accuracy_score(y_test, y_pred)\n\n# # Model Accuracy, how often is the classifier correct?\n# print(\"[XGBoost algorithm] accuracy_score: {:.3f}.\".format(acc))\n# print(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def baseline_model():\n#     model = Sequential()\n#     model.add(Dense(100, input_dim=11911, activation='relu'))\n#     model.add(Dense(50, activation='relu'))\n#     model.add(Dense(4, activation='softmax'))\n#     # compile the keras model\n#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=28, verbose=1)\n# kfold = KFold(n_splits=10, shuffle=True)\n# results = cross_val_score(estimator, X, target_, cv=kfold)\n# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n\n# # Creating a bar plot, displaying only the top k features\n# k=10\n# sns.barplot(x=feature_imp[:10], y=feature_imp.index[:k])\n# # Add labels to your graph\n# plt.xlabel('Feature Importance Score')\n# plt.ylabel('Features')\n# plt.title(\"Visualizing Important Features\")\n# plt.legend()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # List top k important features\n# k=20\n# feature_imp.sort_values(ascending=False)[:k]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Create a selector object that will use the random forest classifier to identify\n# # features that have an importance of more than 0.03\n# sfm = SelectFromModel(clf, threshold=0.03)\n\n# # Train the selector\n# sfm.fit(X_train, y_train)\n\n# feat_labels=X.columns\n\n# # Print the names of the most important features\n# for feature_list_index in sfm.get_support(indices=True):\n#     print(feat_labels[feature_list_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Transform the data to create a new dataset containing only the most important features\n# # Note: We have to apply the transform to both the training X and test X data.\n# X_important_train = sfm.transform(X_train)\n# X_important_test = sfm.transform(X_test)\n\n# # Create a new random forest classifier for the most important features\n# clf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\n# # Train the new classifier on the new dataset containing the most important features\n# clf_important.fit(X_important_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Apply The Full Featured Classifier To The Test Data\n# y_pred = clf.predict(X_test)\n\n# # View The Accuracy Of Our Full Feature Model\n# print('[Randon forest algorithm -- Full feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))\n\n# # Apply The Full Featured Classifier To The Test Data\n# y_important_pred = clf_important.predict(X_important_test)\n\n# # View The Accuracy Of Our Limited Feature Model\n# print('[Randon forest algorithm -- Limited feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_important_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model = XGBClassifier()\n# xgb_model.fit(X_train,y_train)\n\n# # make predictions for test data\n\n# y_pred = model.predict(X_test)\n# # predictions = [round(value) for value in y_pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# type(y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}